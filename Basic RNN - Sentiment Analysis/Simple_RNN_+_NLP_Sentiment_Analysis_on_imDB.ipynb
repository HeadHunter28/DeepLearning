{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXURwZL3gto5Q8WnM+Koqc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HeadHunter28/DeepLearning/blob/main/Basic%20RNN%20-%20Sentiment%20Analysis/Simple_RNN_%2B_NLP_Sentiment_Analysis_on_imDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment analysis on imdb reviews using recurrent neural networks\n",
        "\n",
        "- Building an RNN on imdb dataset from Keras dataset."
      ],
      "metadata": {
        "id": "pCzL2_ik1NJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Importing libraries :"
      ],
      "metadata": {
        "id": "37pVEeQi1UWz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YvlJE35maFty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzS6DqEU07X9"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras.utils import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import SimpleRNN\n",
        "from keras.datasets import imdb\n",
        "from keras import initializers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Setting parameters for text processing :"
      ],
      "metadata": {
        "id": "VZFrmn-P1x4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## The size our vocabulary :\n",
        "max_features = 20000  # This is used in loading the data, picks the most common (max_features) words\n",
        "\n",
        "maxlen = 30  # maximum length of a sequence - truncate after this\n",
        "\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "HPGStrDy15xT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Loading data from TensorFlow :\n",
        "\n",
        "- The function automatically tokenizes the text."
      ],
      "metadata": {
        "id": "oLIdUpcX1Y4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSAI-95R1ftY",
        "outputId": "5d7d292f-a60b-4319-b54a-a519c8c32462"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4fBa5Ez1gGX",
        "outputId": "fc819537-3dbf-45ec-9569-7db634f7652e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25000 train sequences\n",
            "25000 test sequences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#### 4. 1 We have the text already tokenized to the limit of 'max_features' words.\n",
        "\n",
        "- But the tokenized text (which is now a vector) is still in sequences of different length :"
      ],
      "metadata": {
        "id": "xeRTA-Dj1gO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_train[1]))\n",
        "print(len(x_train[2])) #different lengths"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gKUwDpd4wMz",
        "outputId": "f93eabe4-6057-4351-ecbc-ee2374153653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "189\n",
            "141\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2 In order for it to be fed as an input to an RNN, we need to have all sequences (vectors) of same length/dimension.\n",
        "\n",
        "- All sequences must be converted to decided sequence length (maxlen) ^, the sequences smaller than maxlen will be PADDED, while those bigger will be truncated.\n",
        "\n",
        "- The truncation may lead to losing some information, but it is a necessary step.\n",
        "\n",
        "- The task of making sequences of same length done by the **pad_sequences** function of *Keras*, which pads or truncates the sequences based on passed length value."
      ],
      "metadata": {
        "id": "5p3Om4BH1H1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-42C9Ca5N75",
        "outputId": "a2df513b-c1ac-4af9-d02a-e77c592c773a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (25000, 30)\n",
            "x_test shape: (25000, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Now all sequences are of same length.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "1FIp6zwZ5kQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Making an Embedding Layer\n",
        "\n"
      ],
      "metadata": {
        "id": "7tfddE005kaL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The text sequences are of tokenised and of same length, but are in **integer representation** form :"
      ],
      "metadata": {
        "id": "4hRut7955kh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train[2])\n",
        "print(x_train[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1dYEHFA5lk-",
        "outputId": "8c6d050a-3d25-4ff6-c923-2c5794a12f42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  47    6 2307   51    9  170   23  595  116  595 1352   13  191   79\n",
            "  638   89    2   14    9    8  106  607  624   35  534    6  227    7\n",
            "  129  113]\n",
            "[   12  1685   195    25   238    60   796 13713     4   671     7  2804\n",
            "     5     4   559   154   888     7   726    50    26    49  7008    15\n",
            "   566    30   579    21    64  2574]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- To be fed into an RNN layer, we have to convert the integer representation of tokens into **dense vector representations (embeddings)** by mapping each integer index to a corresponding dense vector.\n",
        "\n",
        "- The embedding layer is initialized with an embedding matrix, where each row of the matrix corresponds to the embedding vector for a unique token in the vocabulary.\n",
        "\n",
        "- The number of rows in the matrix is equal to the size of the vocabulary, and the number of columns is equal to the desired dimensionality of the embedding space. Each row represents the embedding vector for a specific token.\n",
        "\n",
        "- Can think of this as learning a word vector embedding \"on the fly\" rather than using an existing mapping (like GloVe)\n",
        "\n",
        "----"
      ],
      "metadata": {
        "id": "sepL5vIG5lr0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####- Using Keras library :\n",
        "\n",
        " #### **keras.layers.embeddings.Embedding** (**input_dim** , **output_dim**, **embeddings_initializer**='uniform' ,\n",
        " #### **embeddings_regularizer** =None , **activity_regularizer**=None, **embeddings_constraint**=None, **mask_zero**=False, **input_length**=None)\n",
        "\n",
        "- This layer maps each integer into a distinct (dense) word vector of length output_dim.\n",
        "\n",
        "- Can think of this as learning a word vector embedding \"on the fly\" rather than using an existing mapping (like GloVe)\n",
        "\n",
        "- The **input_dim** should be the size of the vocabulary.\n",
        "\n",
        "- The **input_length** specifies the length of the sequences that the network expects."
      ],
      "metadata": {
        "id": "vS4RR_rz5mGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "-GYTxqpC5mOY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will feed the following values to the parameters:\n",
        "\n",
        "- For input_dim : size of our vocabulary (max_features) i.e. 100\n",
        "\n",
        "- For output_dim : word_embedding_dim (50)\n",
        "\n",
        " Each integer in the sequence will be taken and embedded in a 50-dimensional vector.\n",
        "\n"
      ],
      "metadata": {
        "id": "JupvVmmV5mnl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Making a Recurrent Layer"
      ],
      "metadata": {
        "id": "vr60AYp75mv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **keras.layers.recurrent.SimpleRNN**(units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)"
      ],
      "metadata": {
        "id": "j6kPKlEEC0lr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The parameter units gives the dimensionality of the output (and therefore the hidden state).\n",
        "\n",
        "- Note that typically there will be another layer after the RNN mapping the (RNN) output to the network output.\n",
        "So we should think of this value as the desired dimensionality of the hidden state and not necessarily the desired output of the network.\n",
        "\n",
        "- We can set the parameters as =\n",
        "\n",
        "    *units* : rnn_hidden_dim (5)\n",
        "\n",
        "    *activation* : 'relu'\n",
        "\n",
        "    *input_shape* : Size of one vector in X_train.shape\n",
        "\n",
        "    *kernel_initializer* : initializers.RandomNormal(stddev=0.001)\n",
        "\n",
        "    *recurrent_initializer* : initializers.Identity(gain=1.0)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ae9J0_2YC0tS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Making a Dense Layer\n",
        "\n",
        "- One node (output is binary)\n",
        "\n",
        "- Will use 'Sigmoid' activation function - preferred for Binary classification"
      ],
      "metadata": {
        "id": "PcF3Px6kC1Jm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "Kn4CV_p6C1RC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Designing the Neural Network\n",
        "\n",
        "- Adding all layers.\n",
        "\n",
        "- Using gradient descent optimiser : RMS Prop\n",
        "\n",
        "- Using loss function : Binary-Cross Entropy"
      ],
      "metadata": {
        "id": "02JgR9wzC1Zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_hidden_dim = 5\n",
        "word_embedding_dim = 50"
      ],
      "metadata": {
        "id": "mGiI4X_FE0y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RNN_model = Sequential()\n",
        "\n",
        "# Sequential Layer\n",
        "RNN_model.add(Embedding(max_features, word_embedding_dim))\n",
        "\n",
        "# Recurrent Layer\n",
        "\n",
        "RNN_model.add(SimpleRNN(rnn_hidden_dim,\n",
        "                    kernel_initializer=initializers.RandomNormal(stddev=0.001),\n",
        "                    recurrent_initializer=initializers.Identity(gain=1.0),\n",
        "                    activation='relu',\n",
        "                    input_shape=x_train.shape[1:]))\n",
        "\n",
        "# Dense Layer\n",
        "\n",
        "RNN_model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "VgVLKvhvE1Oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Let us check all the parameters :"
      ],
      "metadata": {
        "id": "FU_6e4zRE1ZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RNN_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RofEbHZnFwhf",
        "outputId": "6f169b93-ed4a-4e07-f3bc-663992d82a0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, None, 50)          1000000   \n",
            "                                                                 \n",
            " simple_rnn_2 (SimpleRNN)    (None, 5)                 280       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 6         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1000286 (3.82 MB)\n",
            "Trainable params: 1000286 (3.82 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Setting the learning rate, loss function, metrics and compiling the network :"
      ],
      "metadata": {
        "id": "bo53t-9BFwpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rmsprop = keras.optimizers.RMSprop(learning_rate = .0001)\n",
        "\n",
        "RNN_model.compile(loss='binary_crossentropy',\n",
        "              optimizer=rmsprop,\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Myl28KuLFwyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Training the neural network (10 epochs)"
      ],
      "metadata": {
        "id": "FMZKvbmrGUcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RNN_model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=10,\n",
        "          validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuF4zDqMGUkU",
        "outputId": "10d01071-850a-4075-bcbf-a0433a22bc16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "782/782 [==============================] - 16s 19ms/step - loss: 0.6855 - accuracy: 0.5674 - val_loss: 0.6617 - val_accuracy: 0.6262\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 14s 18ms/step - loss: 0.6146 - accuracy: 0.6718 - val_loss: 0.5916 - val_accuracy: 0.6809\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 16s 20ms/step - loss: 0.5461 - accuracy: 0.7261 - val_loss: 0.5475 - val_accuracy: 0.7195\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 14s 17ms/step - loss: 0.4979 - accuracy: 0.7621 - val_loss: 0.5191 - val_accuracy: 0.7388\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 12s 16ms/step - loss: 0.4608 - accuracy: 0.7845 - val_loss: 0.4958 - val_accuracy: 0.7525\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 16s 21ms/step - loss: 0.4332 - accuracy: 0.8014 - val_loss: 0.4869 - val_accuracy: 0.7581\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 16s 20ms/step - loss: 0.4125 - accuracy: 0.8153 - val_loss: 0.4723 - val_accuracy: 0.7694\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 15s 19ms/step - loss: 0.3967 - accuracy: 0.8229 - val_loss: 0.4642 - val_accuracy: 0.7750\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 18s 23ms/step - loss: 0.3850 - accuracy: 0.8276 - val_loss: 0.4635 - val_accuracy: 0.7758\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 20s 26ms/step - loss: 0.3752 - accuracy: 0.8346 - val_loss: 0.4555 - val_accuracy: 0.7817\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7afb46aec4f0>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9.1 Evaluating the neural network's performance"
      ],
      "metadata": {
        "id": "sLcrIlIQGUsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score, acc = RNN_model.evaluate(x_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krgmA0OHGmW9",
        "outputId": "f0091230-f609-4a46-a2cd-ea23eb3740b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 3s 3ms/step - loss: 0.4555 - accuracy: 0.7817\n",
            "Test score: 0.455465167760849\n",
            "Test accuracy: 0.781719982624054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy obtained is 78.17%, which can be improved."
      ],
      "metadata": {
        "id": "QIbw9W8rGmkc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----"
      ],
      "metadata": {
        "id": "mDTQYQuDHhR2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Testing performance with different parameters of:     \n",
        "\n",
        "- **Vocabulary size (max_features)**\n",
        "\n",
        "- **Length of sequences (max_len)**\n",
        "\n",
        "- **Dimensionality of the hidden state (rnn_hidden_state)**\n",
        "\n",
        "- **Epochs** - Training the network for larger number of epochs.\n"
      ],
      "metadata": {
        "id": "oHgE3nsvHhac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10.1 Iteration 1\n",
        "\n",
        "- Vocabulary size : 20000 ( initially 1000)\n",
        "\n",
        "- Sequence length : 80 (initially 50)\n",
        "\n",
        "- Keeping hidden state dimension (5) and embedding layer dimension (50) same."
      ],
      "metadata": {
        "id": "6aW0K-CuHhiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 20000  # This is used in loading the data, picks the most common (max_features) words\n",
        "maxlen = 80  # maximum length of a sequence - truncate after this\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, maxlen=maxlen)"
      ],
      "metadata": {
        "id": "XY24Oib9MHbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Making and compiling the neural network :"
      ],
      "metadata": {
        "id": "URefxIe6M2ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_hidden_dim = 5\n",
        "word_embedding_dim = 50\n",
        "RNN_model1= Sequential()\n",
        "RNN_model1.add(Embedding(max_features, word_embedding_dim))  #This layer takes each integer in the sequence\n",
        "RNN_model1.add(SimpleRNN(rnn_hidden_dim,\n",
        "                    kernel_initializer=initializers.RandomNormal(stddev=0.001),\n",
        "                    recurrent_initializer=initializers.Identity(gain=1.0),\n",
        "                    activation='relu',\n",
        "                    input_shape=x_train.shape[1:]))\n",
        "\n",
        "RNN_model1.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "0bUx0mFSMKiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rmsprop = keras.optimizers.RMSprop(learning_rate = .0001)\n",
        "\n",
        "RNN_model1.compile(loss='binary_crossentropy',\n",
        "              optimizer=rmsprop,\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "-ypCxsQOMPhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Training the neural network :"
      ],
      "metadata": {
        "id": "HiV7ArkFM_Z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RNN_model1.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=10,\n",
        "          validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "id": "xSAN6iehMS31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Evaluating performance :"
      ],
      "metadata": {
        "id": "YE7fp0AdNDyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score, acc = RNN_model1.evaluate(x_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "metadata": {
        "id": "jjHDp5ZANIsI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}